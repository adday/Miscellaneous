{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\Yv}{\\mathbf{Y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\betav}{\\mathbf{\\beta}}\n",
    "\\newcommand{\\gv}{\\mathbf{g}}\n",
    "\\newcommand{\\Hv}{\\mathbf{H}}\n",
    "\\newcommand{\\dv}{\\mathbf{d}}\n",
    "\\newcommand{\\Vv}{\\mathbf{V}}\n",
    "\\newcommand{\\vv}{\\mathbf{v}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\Sv}{\\mathbf{S}}\n",
    "\\newcommand{\\Gv}{\\mathbf{G}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\Zv}{\\mathbf{Z}}\n",
    "\\newcommand{\\Norm}{\\mathcal{N}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "\\newcommand{\\grad}{\\mathbf{\\nabla}}\n",
    "\\newcommand{\\ebx}[1]{e^{\\betav_{#1}^T \\xv_n}}\n",
    "\\newcommand{\\eby}[1]{e^{y_{n,#1}}}\n",
    "\\newcommand{\\Tiv}{\\mathbf{Ti}}\n",
    "\\newcommand{\\Fv}{\\mathbf{F}}\n",
    "\\newcommand{\\ones}[1]{\\mathbf{1}_{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Classification with LDA, QDA, and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alexander Day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is the problem of catergorizing new observations among a set of categories. The goal is to make a \"large\" margin between data to prevent misclassifying new data. A solution to the problem is to use supervised learning methods that use a set of training data where the category is known. In this assignment four such approachs are compared over two data sets. As in other machine learning methods the best approach is often determined by features of the dataset, such as size and distribution of the data. \n",
    "\n",
    "The methods discussed below of solving the classification problem all have nuances that determine when one may be more appropriate than another. As one might expect, the nonlinear methods perform better on data that has a nonlinear distribution and data where linear methods are hampered by the masking problem. Quadratic discminiant analysis is a more flexible method when data is normal. Linear discriminant analysis typically performs better than QDA when data is undersampled or there is a high level of dimensionality to the data.\n",
    "\n",
    "$\\textbf{Quadratic discriminant analysis}$ determines the a new sample is assigned to using a generative model, one that defines a probability disturbution over a range of values. In particular it is a Bayesian classifier that assumes data is normally distributed within a class. Given classes, $C = \\{c_1,...,c_n\\}$, where a vector of attribute values associated to be classified is $\\xv$, the goal is to assign to the sample, $\\xv$, the class $c_i$ if $Pr(C=c_i|\\xv) > Pr(C=c_k|\\xv)$ for every class $k \\neq i$. The quadratic discriminant function, $\\delta_k(\\xv)$, for class $k$ can be derived by starting with Baye's law, normalizing the distribution among a class, and taking logarithms (log-likelihood) to simplify the computations. The derivation produces the following function, called the discriminant function, where the class of a new sample $\\xv$ is $\\argmax{k}\\; \\delta_k(\\xv)$.\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        \\delta_k(\\xv) = -\\frac{1}{2} \\ln |\\Sigma_k| -\\frac{1}{2}(\\xv-\\muv_k)^T\n",
    "\\Sigma_k^{-1} (\\xv-\\muv_k) + \\ln P(C=k)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "$\\textbf{Linear discriminant analysis}$ is very similar to QDA in that it is also uses a generative model that makes use of Baye's law. However, unlike QDA, LDA assumes that the covariance matrix, $\\Sigma_k$, is identical in each class $k$. This is accomplished by using as the covariance matrix the weighted average of class covariance matrices, \n",
    "$\n",
    "\\begin{align*}\n",
    "\\Sigmav = \\sum_{k=1}^K \\frac{N_k}{N} \\Sigmav_k\n",
    "\\end{align*}\n",
    "$. \n",
    "This allows for further reduction in the discriminant function as terms involving only the covariance matrix can be removed. The function determines the class of a new sample as above in quadratic discriminant analysis and is the following.\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        \\delta_k(\\xv) = \\xv^T \\Sigmav^{-1} \\muv_k - \\frac{1}{2}\\muv_k^T \\Sigmav^{-1} \\muv_k + \\ln\n",
    "        P(C=k)\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "$\\textbf{Linear logistic regression}$ is a classification technique that seeks to achieve the highest quality of the output by maximizing the likelihood of the data. Given $n$ classes, a class $k$ is assigned to data sample $\\xv$ if $P(C=k|\\xv_n)$ is a maximum among all $k \\in \\{0,...,n\\}$. The data likelihood to maximize is \n",
    "$$\n",
    "    \\begin{align*}\n",
    "      g_k(\\xv_n) & = P(C=k\\,|\\,\\xv_n) =\n",
    "      \\frac{f(\\xv_n;\\wv_k)}{1+\\sum_{m=1}^{K-1} f(\\xv_n;\\wv_m)}\\\\\n",
    "      f(\\xv_n;\\wv_k) & = \\left \\{ \\begin{array}{ll} \\ebx{k}; & k < K\\\\ 1;& k = K \\end{array} \\right .\\\\\n",
    "      L(\\wv) & = \\prod_{n=1}^N \\prod_{k=1}^K p(C=k\\,|\\, \\xv_n) ^{t_{n,k}}\\\\\n",
    "      & = \\prod_{n=1}^N \\prod_{k=1}^K g_k(\\xv_n)^{t_{n,k}}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Maximizing the data is accomplished by gradient ascent, where the gradient of log likelihood with respect to $\\wv_j$ is\n",
    "$$   \n",
    " \\grad_{\\wv_j} LL(\\wv) = \n",
    " \\sum_{n=1}^N \\xv_n (t_{n,j} - g_j(\\xv_n))\n",
    "$$\n",
    "\n",
    "At each step in the gradient ascent $\\wv_j$ is updated by the rule: \n",
    "$$\n",
    "        \\wv_j  \\leftarrow \\wv_j + \\alpha \\sum_{n=1}^N\n",
    "        (t_{n,j} - g_j(\\xv_n)) \\xv_n\n",
    "$$\n",
    "When the weights have been adjusted appropriately the class assigned to a new sample is the class that has the highest likelihood of containing the sample. \n",
    "\n",
    "\n",
    "$\\textbf{Nonlinear logistic regression}$, similar to linear logistic regression, maximizes the likelihood of the data in assigning a class to a data sample. The difference is that nonlinear logistic regression makes use of a neural network to accomplish gradient ascent and likewise update $\\wv$. The idea in using a neural network is to produce outputs $\\yv$ and perform post processing on these outputs to determine the class to assign to a data sample. We can use the following previously derived expressions for minimizing the squared error in the neural network,\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        \\Zv &= h(\\tilde{\\Xv} \\Vv)\\\\\n",
    "        \\Yv &= \\tilde{\\Zv} \\Wv\\\\\n",
    "      \\end{align*}\n",
    "$$\n",
    "\n",
    "Then the changes needed for nonlinear logistic regression are as follows, where $\\Tiv$\n",
    "is the indicator variable (vector of length $n$ with one nonzero entry) for $\\Tv$,  $\\hat{\\Tiv}$ and $\\hat{\\Gv}$ are matrices for the target indicator variables and the neural network outputs, respectively, with the last column of values for the $K^{th}$ class removed.\n",
    "\n",
    "$$\n",
    "      \\begin{align*}\n",
    "        \\Fv &= [e^{\\Yv}, \\ones{N}] \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\text{ append column of ones}\\\\\n",
    "        \\Sv &= \\Fv \\ones{K-1}\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\text{ sum across columns}\\\\\n",
    "        \\Gv &= \\Fv / \\left [ \\Sv, \\Sv,\\ldots,\\Sv \\right ] \\;\\;\\; \\Sv \\text{ are column vectors }\\\\\n",
    "        LL &=  \\sum \\Tiv \\log \\Gv\\\\\n",
    "        \\grad_\\Vv LL &=    \\tilde{\\Xv}^T \\left ( (\\hat{\\Tiv} - \\hat{\\Gv}) \\hat{\\Wv}^T \\cdot  (1-\\Zv^2) \\right )\\\\\n",
    "        \\grad_\\Wv LL &=   \\tilde{\\Zv}^T (\\hat{\\Tiv} - \\hat{\\Gv})\n",
    "      \\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined below are the following functions that train and evaluate LDA, QDA, and neural network logistic regression models.\n",
    "\n",
    "* `model = trainLDA(X,T,parameters)`\n",
    "* `percentCorrect = evaluateLDA(model,X,T)`\n",
    "* `model = trainQDA(X,T,parameters)`\n",
    "* `percentCorrect = evaluateQDA(model,X,T)`\n",
    "* `model = trainNN(X,T,parameters)`\n",
    "* `percentCorrect = evaluateNN(model,X,T)`\n",
    "\n",
    "The `parameters` argument for `trainNN` is a list of the hidden layers structure and the number of SCG iterations. The value of the `parameters` argument for `trainLDA` and `trainQDA` are not used.\n",
    "\n",
    "Use the `trainValidateTestKFoldsClassification` function in `mlutils.py` to apply the above functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mlutils as ml\n",
    "import neuralnetworks as nn\n",
    "import qdalda as ql\n",
    "\n",
    "def trainQDA( X, T, parameters=None):\n",
    "    model = ql.QDA()\n",
    "    model.train(X,T)\n",
    "    return model\n",
    "    \n",
    "def evaluateQDA( model, X, T):\n",
    "    predicted,_,_ = model.use(X)\n",
    "    return np.sum(predicted.ravel()==T.ravel()) / float(len(T)) * 100\n",
    "\n",
    "def trainLDA( X , T, parameters=None):\n",
    "    model = ql.LDA()\n",
    "    model.train(X,T)\n",
    "    return model\n",
    "    \n",
    "def evaluateLDA( model, X, T):\n",
    "    predicted,_,_ = model.use(X)\n",
    "    return np.sum(predicted.ravel()==T.ravel()) / float(len(T)) * 100 \n",
    "\n",
    "def trainNN( X, T,parameters):\n",
    "    nnet = nn.NeuralNetworkClassifier(X.shape[1],parameters[0],len(np.unique(T)))\n",
    "    return nnet.train(X,T,parameters[1])\n",
    "\n",
    "def evaluateNN( model, X,T):\n",
    "    predicted = model.use(X)\n",
    "    return np.sum(predicted.ravel()==T.ravel()) / float(len(T)) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below allows for formatted output of the the results returned from the above defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printResults(label,results):\n",
    "    print('{:4s} {:>20s}{:>8s}{:>8s}{:>8s}'.format('Algo','Parameters','TrnAcc','ValAcc','TesAcc'))\n",
    "    print('-------------------------------------------------')\n",
    "    for row in results:\n",
    "        # 20 is expected maximum number of characters in printed parameter value list\n",
    "        print('{:>4s} {:>20s} {:7.2f} {:7.2f} {:7.2f}'.format(label,str(row[0]),*row[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Results 1:\n",
    "### Predicting forest cover type in Colorado wilderness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first data set that used the goal is to predict forest cover type from cartographic variables only (no remotely sensed data). The actual forest cover type for a given observation (30 x 30 meter cell) was determined from US Forest Service (USFS) data. Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data. Data is in raw form (not scaled) and contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types). The studied areas include four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices. The dataset is taken from the UCI Machine Learning Repository at https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/. The original dataset includes 581012 instances. Each instance has 54 attributes that predict the class of the cover type, represented by an integer from 1 to 7.The following is a list of all of the attributes:\n",
    "\n",
    "    Elevation (meters) \n",
    "    Aspect  (degrees azimuth) \n",
    "    Slope  (degrees) \n",
    "    Horizontal_Distance_To_Hydrology: Horz Dist to nearest surface water features in meters\n",
    "    Vertical_Distance_To_Hydrology: Vert Dist to nearest surface water features in meters\n",
    "    Horizontal_Distance_To_Roadways: Horz Dist to nearest roadway in meters\n",
    "    Hillshade_9am: Hillshade index at 9am, summer solstice (0 to 255)\n",
    "    Hillshade_Noon: Hillshade index at noon, summer soltice (0 to 255)\n",
    "    Hillshade_3pm: Hillshade index at 3pm, summer solstice (0 to 255) \n",
    "    Horizontal_Distance_To_Fire_Points: Horz Dist to nearest wildfire ignition points in meters\n",
    "    Wilderness_Area (4 binary columns): 0 (absence) or 1 (presence) denoting area designation \n",
    "    Soil_Type (40 binary columns): 0 (absence) or 1 (presence) denoting soil type designation \n",
    "\n",
    "Due to the large size of the dataset (nearly 600,000 instances) it has been reduced to consider only the first 5000 instances. While this is not an ideal thing to do in practice, it greatly reduced computation time especially in the neural networks, where it was found that a high number of iterations produced the best model.\n",
    "\n",
    "A function is defined below to read in the data. No transformation of the downloadable dataset is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeCoverTypeData(filename='covtype_reduced.data'): #change to covtype.data for full dataset\n",
    "    data = np.loadtxt(filename, usecols=range(55), delimiter=',')\n",
    "    print(\"Read\",data.shape[0],\"rows and\",data.shape[1],\"columns from\",filename)\n",
    "    X = data[:,:53]\n",
    "    T = data[:,54:55]\n",
    "    return X,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 5000 rows and 55 columns from covtype_reduced.data\n"
     ]
    }
   ],
   "source": [
    "X,T = makeCoverTypeData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classification method that will be analyzed is LDA. This is done considering 6 folds of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo           Parameters  TrnAcc  ValAcc  TesAcc\n",
      "-------------------------------------------------\n",
      "LDA:                 None   74.04   52.40   57.86\n",
      "LDA:                 None   70.76   50.59   56.26\n",
      "LDA:                 None   71.06   48.88   72.78\n",
      "LDA:                 None   70.74   48.23   74.47\n",
      "LDA:                 None   72.67   51.09   59.38\n",
      "LDA:                 None   74.38   54.95   46.23\n"
     ]
    }
   ],
   "source": [
    "resultsLDA = ml.trainValidateTestKFoldsClassification( trainLDA,evaluateLDA, X,T, [None],\n",
    "                                                       nFolds=6, shuffle=False,verbose=False)\n",
    "printResults('LDA:',resultsLDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While two folds of the data produced test accuracy of great than 70%, the other folds did not do as well. The average test accuracy was 61%. This is certainly better than a random guess over the 7 classes, which would be 14%.\n",
    "\n",
    "Now let's take a look at how well QDA performs over this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo           Parameters  TrnAcc  ValAcc  TesAcc\n",
      "-------------------------------------------------\n",
      "QDA:                 None   77.72   52.35   54.80\n",
      "QDA:                 None   76.26   51.63   62.57\n",
      "QDA:                 None   74.40   48.79   72.23\n",
      "QDA:                 None   75.46   48.63   72.45\n",
      "QDA:                 None   76.47   51.71   64.93\n",
      "QDA:                 None   77.18   54.43   49.10\n"
     ]
    }
   ],
   "source": [
    "resultsQDA = ml.trainValidateTestKFoldsClassification( trainQDA,evaluateQDA, X,T, [None],\n",
    "                                                        nFolds=6, shuffle=False,verbose=False)\n",
    "printResults('QDA:',resultsQDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the six folds that were tested it appears that QDA also did much better on two folds, obtaining a test accuracy of over 70%. Overall for QDA, the test accuracy was 63%, right around the same as for LDA. The inability to achieve a significantly higher level of test accuracy in QDA shows that there is not much advantage or disadvantage in using each covariance matrix, as in QDA, as opposed to averaging across the covariance matrices for each sample.\n",
    "\n",
    "Now let's turn our attention to the logistic classifiers. First let's look at classification using logisitic linear regression with 10, 100, and 500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo           Parameters  TrnAcc  ValAcc  TesAcc\n",
      "-------------------------------------------------\n",
      " NN:            [[0], 10]   74.92   54.61   58.57\n",
      " NN:           [[0], 500]   77.01   52.84   68.87\n",
      " NN:           [[0], 100]   76.63   51.12   73.54\n",
      " NN:           [[0], 500]   76.65   51.96   78.16\n",
      " NN:           [[0], 500]   78.65   55.06   59.72\n",
      " NN:           [[0], 500]   79.63   57.68   52.93\n"
     ]
    }
   ],
   "source": [
    "resultsNN = ml.trainValidateTestKFoldsClassification( trainNN,evaluateNN, X,T, \n",
    "                                                     [[[0], 10], [[0],100], [[0],500]],\n",
    "                                                     nFolds=6, shuffle=False,verbose=False)\n",
    "printResults('NN:',resultsNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this run of trainValidateTestKFoldsClassification it appears that logistic linear regression performs better than QDA and LDA as the average test accuracy is 65%. The higher predictive accuracy in this method gives hope that nonlinear logistic regression may perform even better, as there are possibly nonlinearities in the data that prevent more accurate classification. \n",
    "\n",
    "Now let's look at nonlinear logisitic regression using 100, 500, and 1000 training iterations on a neural network with 10 hidden units. Note, previous (unshown) tests showed that there was not an advantage to deeper networks over this particular data set, and that is why a single hidden layer of 10 units is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo           Parameters  TrnAcc  ValAcc  TesAcc\n",
      "-------------------------------------------------\n",
      " NN:          [[10], 100]   85.11   55.95   58.57\n",
      " NN:          [[10], 100]   84.89   55.89   70.77\n",
      " NN:         [[10], 1000]   87.40   53.30   73.97\n",
      " NN:          [[10], 100]   85.26   52.81   72.68\n",
      " NN:          [[10], 100]   86.82   54.41   65.97\n",
      " NN:          [[10], 500]   88.59   56.54   56.89\n"
     ]
    }
   ],
   "source": [
    "resultsNN = ml.trainValidateTestKFoldsClassification( trainNN,evaluateNN, X,T, \n",
    "                                                     [[[10], 100], [[10],500], [[10],1000]],\n",
    "                                                     nFolds=6, shuffle=False,verbose=False)\n",
    "printResults('NN:',resultsNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above it is clear that the nonlinear logisitic regression was able to perform the best in classifying the data. Here the average test accuracy is 66%, a slight increase over the other methods.\n",
    "\n",
    "To further analyze how the nonlinear logistic regression is performing, let's take a look at the confusion matrix that it produces over a single network of a layer with 10 hidden units trained for 500 iterations over all samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1    2    3    4    5    6    7\n",
      "    ------------------------------------------\n",
      " 1 | 62.8 30.2  0    0    2.3  0.4  4.3   (557 / 557)\n",
      " 2 | 11.8 80.7  0.3  0    5.5  1.5  0.2   (948 / 948)\n",
      " 3 |  0    0.2 84.1  5.8  0   10.0  0     (643 / 643)\n",
      " 4 |  0    0    2.2 97.8  0    0.1  0     (1249 / 1249)\n",
      " 5 |  0    3.9  0    0   95.7  0.4  0     (945 / 945)\n",
      " 6 |  0    0.4 26.9  1.7  0.4 70.6  0     (479 / 479)\n",
      " 7 |  5.0  0    0    0    0    0   95.0   (179 / 179)\n"
     ]
    }
   ],
   "source": [
    "model = trainNN(X,T,[[10],500])\n",
    "predicted = model.use(X)\n",
    "ml.confusionMatrix(T,predicted,range(1,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few interesting points of note that can be observed from the confusion matrix. First, it appears that the neural network is very good at classifying those samples that belong to classes 4, 5, and 7. These are classified with over 95% accuracy, indicating that there is some particular features among these classes that are well represented in the sample attributes. One other point of interest is that there appears to be correlation between class 1 and 2, as well as among class 3 and 6. This is represented by the high number of misclassifications that fall into the other class specifically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Results 2\n",
    "### Predicting eye state from EEG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second data set used to compare the four classification methods attempts  to predict a binary output of whether the eyes of an individual are opened (0) or closed(1). The attributes that determine the class to assign to a sample are 14 continuous-valued EEG measurements recorded with the Emotiv EEG Neuroheadset. The duration of the measurement was 117 seconds. The eye state was detected via a camera during the EEG measurement and added later manually to the file after analysing the video frames. The dataset is taken from http://archive.ics.uci.edu/ml/datasets/EEG+Eye+State. It contains 14,980 instances, all of which are used in the analysis that follows.\n",
    "\n",
    "A function is defined below to read in data. In order to make use of it, the header in the downloadable data must deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeEyeStateData(filename='EEG_eye_state.data'): #change to covtype.data for full dataset\n",
    "    data = np.loadtxt(filename, usecols=range(15), delimiter=',')\n",
    "    print(\"Read\",data.shape[0],\"rows and\",data.shape[1],\"columns from\",filename)\n",
    "    X = data[:,:14]\n",
    "    T = data[:,14:15]\n",
    "    return X,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 14980 rows and 15 columns from EEG_eye_state.data\n"
     ]
    }
   ],
   "source": [
    "X,T = makeEyeStateData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method of classification applied to this data set is LDA. As above, the method is applied over 6 folds of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo           Parameters  TrnAcc  ValAcc  TesAcc\n",
      "-------------------------------------------------\n",
      "LDA:                 None   70.83   56.25   61.43\n",
      "LDA:                 None   69.74   56.82   58.48\n",
      "LDA:                 None   67.11   51.43   71.02\n",
      "LDA:                 None   68.87   55.69   63.35\n",
      "LDA:                 None   69.83   56.57   58.28\n",
      "LDA:                 None   65.45   50.30   83.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/code/python/cs480/a4/qdalda.py:62: RuntimeWarning: overflow encountered in exp\n",
      "  probabilities = np.exp( discriminants - 0.5*D*np.log(2*np.pi) - np.log(np.array(self.prior)) )\n"
     ]
    }
   ],
   "source": [
    "resultsLDA = ml.trainValidateTestKFoldsClassification( trainLDA,evaluateLDA, X,T, [None],\n",
    "                                                       nFolds=6, shuffle=False,verbose=False)\n",
    "printResults('LDA:',resultsLDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo           Parameters  TrnAcc  ValAcc  TesAcc\n",
      "-------------------------------------------------\n",
      "QDA:                 None   77.00   50.81   82.01\n",
      "QDA:                 None   74.58   56.13   80.50\n",
      "QDA:                 None   75.68   57.23   76.70\n",
      "QDA:                 None   79.39   50.08   71.28\n",
      "QDA:                 None   77.75   58.69   63.55\n",
      "QDA:                 None   78.55   62.56   45.66\n"
     ]
    }
   ],
   "source": [
    "resultsQDA = ml.trainValidateTestKFoldsClassification( trainQDA,evaluateQDA, X,T, [None],\n",
    "                                                        nFolds=6, shuffle=False,verbose=False)\n",
    "printResults('QDA:',resultsQDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo           Parameters  TrnAcc  ValAcc  TesAcc\n",
      "-------------------------------------------------\n",
      " NN:           [[0], 100]   71.74   55.24   83.02\n",
      " NN:           [[0], 100]   72.81   56.45   80.40\n",
      " NN:           [[0], 100]   73.77   56.81   77.40\n",
      " NN:           [[0], 100]   75.35   57.56   70.99\n",
      " NN:           [[0], 100]   75.60   59.34   61.48\n",
      " NN:           [[0], 100]   76.57   62.21   44.94\n"
     ]
    }
   ],
   "source": [
    "resultsNN = ml.trainValidateTestKFoldsClassification( trainNN,evaluateNN, X,T, \n",
    "                                                     [[[0], 10], [[0],100], [[0],500]],\n",
    "                                                     nFolds=6, shuffle=False,verbose=False)\n",
    "printResults('NN:',resultsNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-80264ab372c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m resultsNN = ml.trainValidateTestKFoldsClassification( trainNN,evaluateNN, X,T, \n\u001b[0;32m      2\u001b[0m                                                      \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                                                      nFolds=6, shuffle=False,verbose=False)\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprintResults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NN:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresultsNN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alexander/code/python/cs480/a4/mlutils.py\u001b[0m in \u001b[0;36mtrainValidateTestKFoldsClassification\u001b[1;34m(trainf, evaluatef, X, T, parameterSets, nFolds, shuffle, verbose)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mnewXtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXvalidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mnewTtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTvalidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewXtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnewTtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbestParms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[0mtrainAccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluatef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnewXtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnewTtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mtestAccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluatef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-8b5e7ca60eb4>\u001b[0m in \u001b[0;36mtrainNN\u001b[1;34m(X, T, parameters)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrainNN\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mnnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNeuralNetworkClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluateNN\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alexander/code/python/cs480/a4/neuralnetworks.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, T, nIterations, weightPrecision, errorPrecision, verbose)\u001b[0m\n\u001b[0;32m    243\u001b[0m                             \u001b[0mnIterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnIterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                             \u001b[0mftracep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m                             verbose=verbose)\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscgresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alexander/code/python/cs480/a4/scaledconjugategradient.py\u001b[0m in \u001b[0;36mscg\u001b[1;34m(x, f, gradf, *fargs, **params)\u001b[0m\n\u001b[0;32m    145\u001b[0m                 \u001b[0mfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfnew\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                 \u001b[0mgradold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradnew\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m                 \u001b[0mgradnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m                 \u001b[1;31m## If the gradient is zero then we are done.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradnew\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alexander/code/python/cs480/a4/neuralnetworks.py\u001b[0m in \u001b[0;36mgradF\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_multinomialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[0mdVs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdVs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/alexander/code/python/cs480/a4/neuralnetworks.py\u001b[0m in \u001b[0;36m_backward_pass\u001b[1;34m(self, delta, Z)\u001b[0m\n\u001b[0;32m    137\u001b[0m                                  np.dot( Z[Zi-1].T, delta)))\n\u001b[0;32m    138\u001b[0m                 \u001b[0mdVs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                 \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mVi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mZi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdVs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdW\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resultsNN = ml.trainValidateTestKFoldsClassification( trainNN,evaluateNN, X,T, \n",
    "                                                     [[[10], 100], [[10],500], [[0],1000]],\n",
    "                                                     nFolds=6, shuffle=False,verbose=False)\n",
    "printResults('NN:',resultsNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Testing   model = trainLDA(X,T)\n",
      "             accuracy = evaluateLDA(model,X,T)\n",
      "\n",
      "20/20 points. Accuracy is within 10 of correct value 50%\n",
      "\n",
      "   Testing   model = trainQDA(X,T)\n",
      "             accuracy = evaluateQDA(model,X,T)\n",
      "\n",
      "20/20 points. Accuracy is within 10 of correct value 50%\n",
      "\n",
      "   Testing   model = trainNN(X,T, [[5],100])\n",
      "             accuracy = evaluateNN(model,X,T)\n",
      "\n",
      "20/20 points. Accuracy is within 10 of correct value 100%\n",
      "\n",
      "  Testing\n",
      "    resultsNN = ml.trainValidateTestKFoldsClassification( trainNN,evaluateNN, X,T, \n",
      "                                                          [ [ [0], 5], [ [10], 100] ],\n",
      "                                                          nFolds=3, shuffle=False,verbose=False)\n",
      "    bestParms = [row[0] for row in resultsNN]\n",
      "\n",
      "20/20 points. You correctly find the best parameters to be [[10],100] for each fold.\n",
      "\n",
      "C:\\Users\\Alex\\Desktop\\a4 CODING GRADE is 80/80\n",
      "\n",
      "C:\\Users\\Alex\\Desktop\\a4 WRITING GRADE is ??/20\n",
      "\n",
      "C:\\Users\\Alex\\Desktop\\a4 FINAL GRADE is ??/100\n",
      "\n",
      "Remember, this python script is just an example of how your code will be graded.\n",
      "Do not be satisfied with an 80% from running this script.  Write and run additional\n",
      "tests of your own design.\n"
     ]
    }
   ],
   "source": [
    "%run -i A4grader.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
